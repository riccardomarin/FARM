{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Matching.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMg0quR+a6GkX9QKG/vcYeI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/riccardomarin/FARM/blob/master/Matching.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2yRvI6BmMg7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ONLY FOR COLAB\n",
        "# Not required in Binder\n",
        "\n",
        "!wget -c https://repo.anaconda.com/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh\n",
        "!chmod +x Miniconda3-4.5.4-Linux-x86_64.sh\n",
        "!bash ./Miniconda3-4.5.4-Linux-x86_64.sh -b -f -p /usr/local\n",
        "!conda install -q -y --prefix /usr/local python=3.6 ujson\n",
        "\n",
        "import sys\n",
        "sys.path.append('/usr/local/lib/python3.6/site-packages')\n",
        "\n",
        "import ujson\n",
        "print(ujson.dumps({1:2}))\n",
        "\n",
        "!conda install -c conda-forge igl\n",
        "!conda install -c conda-forge meshplot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpI4m_FcmS2V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import igl\n",
        "import scipy\n",
        "import scipy as sp\n",
        "import numpy as np\n",
        "from meshplot import plot, subplot, interact\n",
        "import meshplot\n",
        "from scipy.sparse.linalg import eigs,eigsh\n",
        "from scipy.sparse import csr_matrix\n",
        "import os \n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "!pip install tensorflow\n",
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rBPjw-0mXTL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_pair(v1, v2, f1, f2, c1, c2, color_ops = {}):\n",
        "    # Compute a scale factor\n",
        "    M1 = igl.massmatrix(v1, f1, igl.MASSMATRIX_TYPE_VORONOI)\n",
        "    M2 = igl.massmatrix(v2, f2, igl.MASSMATRIX_TYPE_VORONOI)\n",
        "    scale_factor = np.sqrt(np.sum(M2)/np.sum(M1))\n",
        "\n",
        "    # Align the shapes\n",
        "    v2 = v2 - np.mean(v2,axis=0)\n",
        "    v1_align = v1 * scale_factor + np.mean(v1,axis=0) + [0.7,-0.7,0.0]\n",
        "\n",
        "    # Merge the models\n",
        "    v_all = np.vstack((v1_align, v2))\n",
        "    f_all = np.vstack((f1, f2 + np.max(f1)+1))\n",
        "    c_all = np.vstack((c1, c2))\n",
        "    \n",
        "    plot(v_all, f_all, c_all, shading = color_ops)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGY1lpZGai3v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load Shapes\n",
        "v_src, f_src = igl.read_triangle_mesh(os.path.join('.', \"data\", \"tr_reg_089.off\"))\n",
        "\n",
        "L_src = -igl.cotmatrix(v_src, f_src)\n",
        "M_src = igl.massmatrix(v_src, f_src, igl.MASSMATRIX_TYPE_VORONOI)\n",
        "\n",
        "try:\n",
        "    evals_src, evecs_src = eigsh(L_src, 200, M_src, sigma=0.0, which='LM', maxiter=1e9, tol=1.e-15)\n",
        "except:\n",
        "    evals_src, evecs_src = eigsh(L_src- 1e-8* scipy.sparse.identity(v_src.shape[0]), 200,\n",
        "                         M_src, sigma=0.0, which='LM', maxiter=1e9, tol=1.e-15)\n",
        "\n",
        "evals_src = evals_src.astype(np.float32)\n",
        "evals_src = evals_src.astype(np.float32)\n",
        "\n",
        "v_tar, f_tar = igl.read_triangle_mesh(os.path.join('.', \"data\", \"tr_reg_090.off\"))\n",
        "\n",
        "L_tar = -igl.cotmatrix(v_tar, f_tar)\n",
        "M_tar = igl.massmatrix(v_tar, f_tar, igl.MASSMATRIX_TYPE_VORONOI)\n",
        "\n",
        "try:\n",
        "    evals_tar, evecs_tar = eigsh(L_tar, 200, M_tar, sigma=0.0, which='LM', maxiter=1e9, tol=1.e-15)\n",
        "except:\n",
        "    evals_tar, evecs_tar = eigsh(L_tar- 1e-8* scipy.sparse.identity(v_tar.shape[0]), 200,\n",
        "                         M_src, sigma=0.0, which='LM', maxiter=1e9, tol=1.e-15)\n",
        "    \n",
        "evals_src = evals_tar.astype(np.float32)\n",
        "evals_src = evals_tar.astype(np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXJB3W56qLKr",
        "colab_type": "text"
      },
      "source": [
        "# WKS\n",
        "\n",
        "We will use the Wave Kernel Signature (WKS) descriptor to do the matching. Recall the formula:\n",
        "\n",
        "$K_E(x,x) = \\sum\\limits_{l=1}^{\\infty}e^{- \\frac{(log(E) - log(\\lambda_l))^2}{2\\sigma^2}} \\phi_l(x)^2 $\n",
        "\n",
        "Where:\n",
        "- $sigma = 7 \\delta$\n",
        "- $delta =  (e_{max} - e{min})/ M$\n",
        "- $e_{max} = log(E_N) - 2\\sigma$\n",
        "- $e_{min} = log(E_1) + 2\\sigma$\n",
        "- $E_N$ is the max eigenvalue in absolute value\n",
        "- $E_1$ is the min non-zero eigenvalue in absolute value\n",
        "- $M$ is the number of WKS scales\n",
        "\n",
        "The tasks are:\n",
        "- Read the meshes, compute the LBO eigenvectors\n",
        "- Define the WKS computation\n",
        "- Visualize the WKS scales on meshes\n",
        "- Perform the matching using WKS (Nearest-Neighbor in the descriptor space)\n",
        "- Visualize the matching (and compute the error)\n",
        "\n",
        "Are the descriptors coherent among the shapes, for different descriptor scales? Is the matching good? We can change the number of descriptors: does it impact the matching?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xa35jY3hpwW5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def WKS(vertices, faces, evals, evecs, wks_size, variance):\n",
        "    # Number of vertices\n",
        "    n = vertices.shape[0]\n",
        "    WKS = np.zeros((n,wks_size))\n",
        "\n",
        "    # Just for numerical stability\n",
        "    evals[evals<1e-6] = 1e-6\n",
        "\n",
        "    # log(E)\n",
        "    log_E = np.log(evals).T\n",
        "\n",
        "    # Define the energies step\n",
        "    e = np.linspace(log_E[1], np.max(log_E)/1.02, wks_size)\n",
        "\n",
        "    # Compute the sigma\n",
        "    sigma = (e[1]-e[0]) * variance\n",
        "    C = np.zeros((wks_size,1))\n",
        "\n",
        "    for i in np.arange(0,wks_size):\n",
        "        # Computing WKS\n",
        "        WKS[:,i] = np.sum(\n",
        "            (evecs)**2 * np.tile( np.exp((-(e[i] - log_E)**2) / (2*sigma**2)),(n,1)), axis=1)\n",
        "        \n",
        "        # Noramlization\n",
        "        C[i] = np.sum(np.exp((-(e[i]-log_E)**2)/(2*sigma**2)))\n",
        "        \n",
        "    WKS = np.divide(WKS,np.tile(C,(1,n)).T)\n",
        "    return WKS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBG2ZKlHp1Jv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Computing the descriptors for the two shapes\n",
        "d_src = WKS(v_src, f_src, evals_src, evecs_src, 100, 7)\n",
        "d_tar = WKS(v_tar, f_tar, evals_tar, evecs_tar, 100, 7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sfu49Cf01Xec",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visualizing descriptors\n",
        "i = 2\n",
        "plot_pair(v_src, v_tar, f_src, f_tar, d_src[:,i:i+1], d_tar[:,i:i+1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWHhseWx1ZDm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Nearest Neighbor\n",
        "treesearch = sp.spatial.cKDTree(d_tar)\n",
        "p2p = treesearch.query(d_src, k=1)[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyRvBccW1aNc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To see the quality of the matching we plot a function on one shape and we transfer it to the other\n",
        "funz_ = (v_tar - np.min(v_tar,0))/np.tile((np.max(v_tar,0)-np.min(v_tar,0)),(np.size(v_tar,0),1));\n",
        "colors = np.cos(funz_);\n",
        "funz_tar = (colors-np.min(colors))/(np.max(colors) - np.min(colors));\n",
        "funz_src = funz_tar[p2p]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gN3sxDPK1euO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot and (euclidean) error evaluation\n",
        "funz_src = funz_tar[p2p]\n",
        "plot_pair(v_src, v_tar, f_src, f_tar, funz_src,funz_tar)\n",
        "err = np.sum(np.square(v_tar - v_tar[p2p,:]))\n",
        "print(err)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkmqC8jZ59S-",
        "colab_type": "text"
      },
      "source": [
        "# Functional Maps\n",
        "\n",
        "Here we will use the Functional Maps framework. Given some descriptor $D$ on the first shape and $F$ on the second, we will compute our map $C$ such that: \\\n",
        "$ d = (\\phi_{src}^{T} A_{src} d) $ \\\n",
        "$ f = (\\phi_{tar}^{T} A_{tar} f) $ \\\n",
        "$ C D = F $ \\\n",
        "$ C =  F D^{-1} $"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTyopAGD583y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# WKS\n",
        "n_evals = 5\n",
        "n_desc = 3\n",
        "D = (np.matmul(evecs_src[:,0:n_evals].T, np.matmul(M_src.todense() , d_src[:,0:n_desc])))\n",
        "F = (np.matmul(evecs_tar[:,0:n_evals].T, np.matmul(M_tar.todense() , d_tar[:,0:n_desc])))\n",
        "C =  np.matmul(F,np.linalg.pinv(D))\n",
        "\n",
        "plt.imshow(C)\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObnDcw1x-Ytg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute p2p correspondence\n",
        "treesearch = sp.spatial.cKDTree(np.matmul(evecs_tar[:,0:n_evals],C))\n",
        "p2p = treesearch.query(evecs_src[:,0:n_evals], k=1)[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgk5NvZOAEKa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "funz_src = funz_tar[p2p]\n",
        "plot_pair(v_src, v_tar, f_src, f_tar, funz_src,funz_tar)\n",
        "err = np.sum(np.square(v_tar - v_tar[p2p,:]))\n",
        "print(err)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "divV5v4v3cl2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# landmarks\n",
        "n_land = 45\n",
        "n_evals = 20\n",
        "step = np.int(np.ceil(v_src.shape[0] / n_land))\n",
        "a = np.arange(0,v_src.shape[0],step)\n",
        "\n",
        "landmarks = np.zeros((v_src.shape[0], a.size))\n",
        "landmarks[a,np.arange(a.size)] = 1\n",
        "\n",
        "D = (np.matmul(evecs_src[:,0:n_evals].T, np.matmul(M_src.todense() , landmarks[:,0:n_land]))).T\n",
        "F = (np.matmul(evecs_tar[:,0:n_evals].T, np.matmul(M_tar.todense() , landmarks[:,0:n_land]))).T\n",
        "C =  np.matmul(np.linalg.pinv(D),F)\n",
        "\n",
        "plt.imshow(C)\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-d9okpkc5Lzi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "treesearch = sp.spatial.cKDTree(np.matmul(evecs_tar[:,0:n_evals],C))\n",
        "p2p = treesearch.query(evecs_src[:,0:n_evals], k=1)[1]\n",
        "funz_src = funz_tar[p2p]\n",
        "plot_pair(v_src, v_tar, f_src, f_tar, funz_src,funz_tar)\n",
        "err = np.sum(np.square(v_tar - v_tar[p2p,:]))\n",
        "print(err)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ttg0B3wy-VCq",
        "colab_type": "text"
      },
      "source": [
        "Here we implement Functional Maps as an optimization problem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piGDB7xDAOnC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Computing descriptors\n",
        "n_land =  20\n",
        "n_wks  =  2\n",
        "\n",
        "n_evals = 20\n",
        "\n",
        "# Landmarks\n",
        "step = np.int(np.ceil(v_src.shape[0] / n_land))\n",
        "a = np.arange(0,v_src.shape[0],step)\n",
        "landmarks = np.zeros((v_src.shape[0], a.size))\n",
        "landmarks[a,np.arange(a.size)] = 1\n",
        "\n",
        "# WKS\n",
        "d_src = WKS(v_src, f_src, evals_src, evecs_src, n_wks, 7)\n",
        "d_tar = WKS(v_tar, f_tar, evals_tar, evecs_tar, n_wks, 7)\n",
        "\n",
        "# Optimization Process\n",
        "desc_src = np.hstack((landmarks,d_src))\n",
        "desc_tar = np.hstack((landmarks,d_tar))\n",
        "\n",
        "# Descriptor normalization\n",
        "no = np.sqrt(np.diag(np.matmul(M_src.T.__matmul__(desc_src).T, desc_src)))\n",
        "no_s = np.tile(no.T,(v_src.shape[0],1))\n",
        "no_t = np.tile(no.T,(v_tar.shape[0],1))\n",
        "fct_src = np.divide(desc_src,no_s)\n",
        "fct_tar = np.divide(desc_tar,no_t)\n",
        "\n",
        "# Coefficents of the obtained descriptors\n",
        "Fct_src = np.matmul(M_src.T.__matmul__(evecs_src[:, 0:n_evals]).T, fct_src)\n",
        "Fct_tar = np.matmul(M_tar.T.__matmul__(evecs_tar[:, 0:n_evals]).T, fct_tar)\n",
        "\n",
        "# The relation between the two constant functions can be computed in a closed form\n",
        "constFct = np.zeros((n_evals,1))\n",
        "constFct[0, 0] = np.sign(evecs_src[0, 0] * evecs_tar[0, 0]) * np.sqrt(np.sum(M_tar)/np.sum(M_src))\n",
        "\n",
        "# Different way to compute Laplacian commutativity\n",
        "# Dlb = (np.tile(evals_src[0:n_evals], (n_evals, 1)) - np.tile(evals_tar[0:n_evals].T, (n_evals, 1)))**2\n",
        "# Dlb = np.float32(Dlb/tf.reduce_sum((Dlb**2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUVLx9_4NhhR",
        "colab_type": "text"
      },
      "source": [
        "Play with different energy weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjSwxIYJ5e0y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Energy weights\n",
        "a = 1e-1 # Descriptors preservation\n",
        "c = 1e-8 # Commutativity with Laplacian\n",
        "\n",
        "# Optimizer\n",
        "adam = tf.keras.optimizers.Adam(1e-1) # Optimization technique\n",
        "\n",
        "# Define tensorflow objects\n",
        "fs = tf.constant(Fct_src, dtype=tf.float32)\n",
        "ft = tf.constant(Fct_tar, dtype=tf.float32)\n",
        "evals = tf.constant(tf.linalg.tensor_diag(np.reshape(np.float32(evals_src[0:n_evals]), (n_evals,))), dtype=tf.float32)\n",
        "evalt = tf.constant(tf.linalg.tensor_diag(np.reshape(np.float32(evals_tar[0:n_evals]), (n_evals,))), dtype=tf.float32)\n",
        "\n",
        "# Initialize C\n",
        "C_ini = np.zeros((n_evals,n_evals))\n",
        "C_ini[0,0]=constFct[0,0]\n",
        "C = tf.Variable(tf.zeros((n_evals,n_evals), dtype=tf.float32))\n",
        "C.assign(C_ini)\n",
        "trainable_vars = [C]\n",
        "\n",
        "for i in np.arange(0,1000):\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss1 = a * tf.reduce_sum(((tf.matmul(C, fs) - ft) ** 2)) / 2\n",
        "        loss2 = c * tf.reduce_sum((tf.matmul(C, evals) - tf.matmul(evalt,C))**2) #tf.reduce_sum(((C ** 2) * Dlb) / 2)\n",
        "        #loss3 = 1e-6 *  tf.reduce_sum(tf.square(tf.matmul(tf.transpose(C),C) - tf.eye(n_evals)))\n",
        "        loss = loss1  + loss2# + loss3\n",
        "\n",
        "    # Apply gradient\n",
        "    grad = tape.gradient(loss,trainable_vars)\n",
        "    tmp = adam.apply_gradients(zip(grad,trainable_vars))\n",
        "\n",
        "C = C.numpy()\n",
        "plt.imshow(C)\n",
        "plt.colorbar()\n",
        "plt.show()\n",
        "\n",
        "treesearch = sp.spatial.cKDTree(np.matmul(evecs_tar[:,0:n_evals],C))\n",
        "p2p = treesearch.query(evecs_src[:,0:n_evals], k=1)[1]\n",
        "funz_src = funz_tar[p2p]\n",
        "plot_pair(v_src, v_tar, f_src, f_tar, funz_src,funz_tar)\n",
        "err = np.sum(np.square(v_src - v_src[p2p,:]))\n",
        "print(err)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mhc2WSUQS8Xw",
        "colab_type": "text"
      },
      "source": [
        "A common post-processing is ICP refinement. The main idea is that we can think to our spectral embedding as N-dimensional pointclouds. If my change of basis $C$ brings the two close enough, we can try to improve the alignment using the rigid registration algorithm called Iterative Closest Point (ICP). \n",
        "It is based on two steps. Given a pointcloud $M$ that I want to align to $N$:\n",
        "- Computing the Nearest Neighbor matching between $M$ and $N$\n",
        "- Computing the optimal roto-translation via SVD that align $M$ with $N$ using the given NN correspondence\n",
        "- Iterate until convergence\n",
        "\n",
        "It is prone to get stuck in local-minima, but if the pointclouds are alrady close enough, it can significatly improve the solution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zv-jXEQZ8BHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('ICP refine...')\n",
        "for k in np.arange(0,5):\n",
        "    treesearch = scipy.spatial.cKDTree(np.matmul(evecs_tar[:,0:n_evals],C))\n",
        "    matches = treesearch.query(evecs_src[:,0:n_evals], k=1)[1]\n",
        "    W = np.linalg.lstsq(evecs_src[matches, 0:n_evals],evecs_tar[:, 0:n_evals])[0]\n",
        "    d = np.linalg.svd(W)\n",
        "    C_ICP = np.matmul(np.matmul(d[0], np.eye(n_evals)), d[2])\n",
        "\n",
        "plt.imshow(C_ICP)\n",
        "plt.colorbar()\n",
        "plt.show()\n",
        "\n",
        "treesearch = sp.spatial.cKDTree(evecs_tar[:,0:n_evals])\n",
        "p2p = treesearch.query(np.matmul(evecs_src[:,0:n_evals], C_ICP), k=1)[1]\n",
        "funz_src = funz_tar[p2p]\n",
        "plot_pair(v_src, v_tar, f_src, f_tar, funz_src,funz_tar)\n",
        "err = np.sum(np.square(v_tar - v_tar[p2p,:]))\n",
        "print(err)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcutF7d3UOdz",
        "colab_type": "text"
      },
      "source": [
        "Finally, here we anticipate the ZoomOut refinement that you will see tomorrow. This let to compute a high dimensional $C$. It is based on two iterative steps.Given a $C$ of dimension $n \\times n$:\n",
        "- Convert it into a dense correspondence\n",
        "- Convert the dense correspondence into a $C$ of dimension $(n+1) \\times (n+1)$ \n",
        "These two simple steps produce an impressive improvement in the matching quality."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQkuKj10UKld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "C_iter = C_ICP\n",
        "n_iter = 20\n",
        "for i in np.arange(0,n_iter):\n",
        "  treesearch = sp.spatial.cKDTree(np.matmul(evecs_tar[:,0:n_evals+i], C_iter))\n",
        "  p2p = treesearch.query(evecs_src[:,0:n_evals+i], k=1)[1]\n",
        "  C_iter = np.matmul(np.linalg.pinv(evecs_src[:,0:n_evals+i+1]),evecs_tar[p2p,0:n_evals+i+1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYcTv7aeVMUy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.imshow(C_iter)\n",
        "plt.colorbar()\n",
        "plt.show()\n",
        "treesearch = sp.spatial.cKDTree(np.matmul(evecs_tar[:,0:n_evals+n_iter],C_iter))\n",
        "p2p = treesearch.query(evecs_tar[:,0:n_evals+n_iter], k=1)[1]\n",
        "funz_src = funz_tar[p2p]\n",
        "plot_pair(v_src, v_tar, f_src, f_tar, funz_src,funz_tar)\n",
        "err = np.sum(np.square(v_tar - v_tar[p2p,:]))\n",
        "print(err)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}